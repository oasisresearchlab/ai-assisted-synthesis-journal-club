# Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations

**Authors:** Xin Quan, Marco Valentino, Louise A. Dennis, Andr√© Freitas
**Year:** 2025
**DOI:** 10.48550/arXiv.2505.24264
**Citation Count:** 1

## Evidence Items

- The proposed LLM-theorem prover framework achieved substantial improvements in autoformalisation faithfulness across three NLI datasets, with gains of +18.46% on e-SNLI, +34.2% on QASC, and +39.77% on WorldTree compared to the baseline. #evd-candidate ^evd-001
	- **What**: Percentage improvements in autoformalisation faithfulness (ability to convert natural language explanations to formal logical representations)
	- **How**: Comparative evaluation between proposed framework and Explanation-Refiner baseline using manual assessment of formalised logical forms
	- **Who**: Multiple LLMs (GPT-4o, GPT-4o-mini, Deepseek-V3, Llama3.1) tested on e-SNLI, QASC, and WorldTree datasets
- The framework demonstrated significant improvements in explanation refinement rates, achieving +29.5%, +51.5%, and +41.25% gains on e-SNLI, QASC, and WorldTree respectively compared to the state-of-the-art baseline. #evd-candidate ^evd-002
	- **What**: Percentage improvements in explanation refinement rates (successful correction of logical errors in natural language explanations)
	- **How**: Comparative study measuring refinement success rates between proposed approach and Explanation-Refiner baseline
	- **Who**: Multiple LLMs tested across three standard NLI explanation datasets (e-SNLI, QASC, WorldTree)
- The proposed framework substantially reduced computational efficiency requirements, decreasing the average number of LLM calls by 58.60% on e-SNLI, 39.39% on QASC, and 31.15% on WorldTree across all tested models. #evd-candidate ^evd-003
	- **What**: Percentage reduction in computational calls required for successful theorem proving verification
	- **How**: Counting and comparing average LLM API calls needed between proposed framework and baseline approach
	- **Who**: All tested LLMs (GPT-4o, GPT-4o-mini, Deepseek-V3, Llama3.1) across three NLI datasets
- Specific LLM models showed dramatic differences in refinement rate performance, with the proposed approach achieving 78% vs 51% for Llama3.1, 95% vs 69% for Deepseek-V3, and 77% vs 30% for GPT-4o-mini on e-SNLI. #evd-candidate ^evd-004
	- **What**: Head-to-head refinement rate percentages comparing proposed framework vs Explanation-Refiner baseline
	- **How**: Direct comparison of successful explanation refinement percentages achieved by each approach per model
	- **Who**: Three specific LLMs (Llama3.1, Deepseek-V3, GPT-4o-mini) evaluated on e-SNLI dataset
- Ablation studies revealed that detailed feedback and syntax error refinement components had the highest impact on performance, with their removal causing the most significant drop in framework effectiveness. #evd-candidate ^evd-005
	- **What**: Performance degradation measurements when removing specific architectural components
	- **How**: Systematic ablation study removing individual components and measuring resulting performance changes
	- **Who**: All tested LLMs across the three NLI datasets with component-wise performance analysis
- The framework showed superior handling of complex inference scenarios, with particularly strong performance improvements on WorldTree dataset which contains more challenging material inferences. #evd-candidate ^evd-006
	- **What**: Relative performance gains on datasets of varying complexity, with WorldTree representing more challenging scenarios
	- **How**: Cross-dataset performance comparison measuring refinement success on datasets with different inference complexity levels
	- **Who**: Multiple LLMs tested on three datasets with WorldTree identified as containing more complex material inferences
- Most syntactic errors in the autoformalisation process were corrected within the first three iterations, after which improvement rates stabilized, indicating predictable error correction patterns. #evd-candidate ^evd-007
	- **What**: Iteration-by-iteration tracking of syntactic error reduction rates during the refinement process
	- **How**: Empirical analysis tracking error correction success across multiple refinement iterations
	- **Who**: All tested LLM models during the explanation refinement process across the three NLI datasets
- Variable and quantifier errors were identified as the most significant factors impacting autoformalisation faithfulness, representing key weaknesses in LLM logical reasoning capabilities. #evd-candidate ^evd-008
	- **What**: Error type analysis showing specific categories of logical formalization failures
	- **How**: Manual evaluation and categorization of autoformalisation errors to identify most problematic error types
	- **Who**: LLM-generated formal representations across e-SNLI, QASC, and WorldTree datasets
