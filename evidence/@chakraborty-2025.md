# Empirical Evaluation of Prompting Strategies for Fact Verification Tasks

**Authors:** Mohna Chakraborty, Adithya Kulkarni, Qi Li
**Year:** 2025
**DOI:** 10.1145/3701716.3717815
**Citation Count:** 1

## Evidence Items

- GPT-3.5 achieved over 72% Label Accuracy on fact verification tasks when using optimized prompting strategies #evd-candidate ^evd-001
	- **What**: Label Accuracy (LA) metric measuring binary classification performance on claim-evidence pairs
	- **How**: Comparative evaluation of three different prompting strategies on standardized fact verification benchmark
	- **Who**: GPT-3.5 commercial LLM tested on FEVER 1.0 dataset containing Wikipedia claims
- Carefully crafted prompts significantly outperformed simpler prompting approaches for the same AI system #evd-candidate ^evd-002
	- **What**: Comparative performance differences between prompting strategies measured by Label Accuracy
	- **How**: Direct comparison of three prompt designs applied to identical fact verification tasks
	- **Who**: Single GPT-3.5 model tested across different prompting conditions on FEVER 1.0
- Approximately 70% of AI system errors in fact verification stem from logical reasoning and contextual misunderstanding failures #evd-candidate ^evd-003
	- **What**: Error categorization and frequency analysis of incorrect predictions
	- **How**: Detailed error analysis of failed cases from the best-performing prompt condition
	- **Who**: GPT-3.5 prediction errors analyzed from FEVER 1.0 fact verification task performance
- Commercial LLMs demonstrate measurable effectiveness for fact verification tasks when properly configured #evd-candidate ^evd-004
	- **What**: Binary classification accuracy exceeding 70% threshold on standardized benchmark
	- **How**: Controlled experimental evaluation using established fact verification dataset and metrics
	- **Who**: GPT-3.5 as representative commercial LLM tested on FEVER 1.0 Wikipedia-based claims
- Prompt engineering strategies show substantial impact on AI system performance for entailment tasks #evd-candidate ^evd-005
	- **What**: Performance variance across different prompting approaches measured by accuracy differences
	- **How**: Systematic comparison of prompt designs while controlling for model and dataset variables
	- **Who**: GPT-3.5 evaluated with three distinct prompting strategies on FEVER 1.0 benchmark
