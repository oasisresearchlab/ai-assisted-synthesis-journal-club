# Retrieval Augmented Fact Verification by Synthesizing Contrastive Arguments

**Authors:** Zhenrui Yue, Huimin Zeng, Lanyu Shang, Yifan Liu, Yang Zhang, Dong Wang
**Year:** 2024
**DOI:** 10.48550/arXiv.2406.09815
**Citation Count:** 17

## Evidence Items

- RAFTS with GPT-3.5 achieved a 12.0% performance gain in F1 score compared to the best baseline method for fact verification #evd-candidate ^evd-001
	- **What**: F1 score improvements measured across multiple fact-checking datasets with multi-class classification outputs
	- **How**: Comparative evaluation using macro recall, precision, and F1 scores against supervised baselines (dE-FEND, SentHAN, SBERT-FC, CofCED) and other GPT-3.5-based methods
	- **Who**: LIAR, RAWFC, and ANTiVax datasets with varying classification granularity (2-6 classes) for claim verification
- RAFTS with Mistral 7B (a smaller 7B parameter model) outperformed the best baseline method by 8.8% in F1 score despite being significantly smaller than GPT-3.5 #evd-candidate ^evd-002
	- **What**: F1 score performance metrics comparing smaller LLM against larger models and supervised baselines
	- **How**: In-context learning approach with retrieval augmentation compared against traditional supervised methods and larger LLMs
	- **Who**: Same fact-checking datasets (LIAR, RAWFC, ANTiVax) used to test model size efficiency trade-offs
- Document retrieval component showed consistent performance improvements of 3.56% average across all metrics and datasets, with up to 37.61% improvement in Recall@5 on Check-COVID dataset #evd-candidate ^evd-003
	- **What**: Recall@5, precision, and other retrieval metrics measuring relevant document identification accuracy
	- **How**: Two-stage coarse-to-fine retrieval pipeline compared against baseline retrieval methods like BM25
	- **Who**: MS MARCO and Check-COVID datasets for document retrieval evaluation, with Wikipedia and scholarly articles as evidence sources
- Both RAFTS variants and baselines achieved high explanation quality scores above 0.9 for politeness and factuality in generated explanations #evd-candidate ^evd-004
	- **What**: Automated evaluation scores for explanation quality including politeness, factuality, and claim-relevance metrics
	- **How**: Automated evaluation framework assessing the quality of natural language explanations generated alongside fact-checking predictions
	- **Who**: All tested datasets (LIAR, RAWFC, ANTiVax) for explanation generation quality assessment
- RAFTS consistently outperformed GPT-3.5-based methods (GPT-3.5, CoT, ReAct, HiSS) across multiple fact-checking datasets without requiring complex prompting techniques #evd-candidate ^evd-005
	- **What**: Comparative performance metrics across multiple GPT-3.5 based approaches measuring classification accuracy
	- **How**: Simple in-context learning with retrieved examples and documents compared against complex prompting strategies like chain-of-thought and ReAct
	- **Who**: Benchmark comparison across LIAR, RAWFC, and ANTiVax datasets testing various LLM-based fact-checking approaches
- Performance deterioration was observed when document retrieval domain differs significantly from the fact-checking domain, indicating domain adaptation challenges #evd-candidate ^evd-006
	- **What**: Performance degradation measurements when cross-domain retrieval is used (e.g., Wikipedia for COVID claims)
	- **How**: Cross-domain evaluation comparing within-domain vs. cross-domain document retrieval effectiveness
	- **Who**: Implicit evaluation across different domain combinations, particularly noted for COVID-related fact-checking using non-medical document sources
- Supervised baseline methods (dE-FEND, SentHAN, SBERT-FC, CofCED) were consistently outperformed by the retrieval-augmented approach across all tested datasets #evd-candidate ^evd-007
	- **What**: Classification performance metrics comparing traditional supervised learning approaches against retrieval-augmented methods
	- **How**: Head-to-head comparison using identical evaluation metrics (macro recall, precision, F1) across same test datasets
	- **Who**: All four supervised baseline models tested on LIAR, RAWFC, and ANTiVax datasets for comprehensive comparison
