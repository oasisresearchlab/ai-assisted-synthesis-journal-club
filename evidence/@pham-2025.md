# ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM

**Authors:** Hoang Pham, Thanh-Do Nguyen, Khac-Hoai Nam Bui
**Year:** 2025
**DOI:** 10.48550/arXiv.2505.22552
**Citation Count:** 1

## Evidence Items

- ClaimPKG achieved state-of-the-art performance with 9-12% accuracy improvements over strong baselines across multiple claim verification categories #evd-candidate ^evd-001
	- **What**: Accuracy percentages measured across different claim categories, with ClaimPKG showing 9-12 percentage point improvements
	- **How**: Comparative evaluation against baseline methods (Zero-shot CoT, GEAR, KG-GPT) using accuracy as primary metric on balanced test samples
	- **Who**: FactKG dataset with 108K claims from DBpedia, evaluated on 2K randomly sampled claims from test set
- Larger General LLMs outperformed smaller ones by up to 8 accuracy points in claim verification tasks #evd-candidate ^evd-002
	- **What**: Accuracy scores showing up to 8 percentage point difference between large models (GPT-4o-Mini, Llama-3.3-70B) versus smaller models (Qwen-2.5-7B, Llama-3.1-8B)
	- **How**: Systematic comparison of different model sizes (7B to 70B parameters) using retrieved KG triplets as input
	- **Who**: Various LLM model families including Llama, Qwen, and GPT series tested on FactKG dataset
- ClaimPKG demonstrated zero-shot generalizability to unstructured datasets beyond its training domain #evd-candidate ^evd-003
	- **What**: Transfer performance metrics on datasets requiring multi-hop reasoning and evidence aggregation from Wikipedia
	- **How**: Zero-shot evaluation using DBpedia-trained models on datasets with different structures and evidence sources
	- **Who**: HoVer and FEVEROUS datasets, using trained Specialized LLMs (Llama-3.2-3B and Qwen-2.5-3B) with Llama-3.3-70B as General LLM
- Specific ClaimPKG configurations achieved accuracy scores of 81.05% to 85.22% depending on LLM combinations #evd-candidate ^evd-004
	- **What**: Exact accuracy percentages: 81.05% (Llama-3B* + GPT-4o-mini), 85.22% (Llama-3B* + Qwen-72B), 84.64% (Llama-3B* + Llama-70B)
	- **How**: End-to-end evaluation combining specialized LLM for pseudo-subgraph generation with different general-purpose LLMs for reasoning
	- **Who**: FactKG dataset claims evaluated using different combinations of fine-tuned specialized models and large general models
- Optimal hyperparameters for subgraph retrieval were empirically determined as k1=3 and k2=1 for balancing performance and computational efficiency #evd-candidate ^evd-005
	- **What**: Performance metrics and computational cost measurements across different hyperparameter settings, with peak performance at specific k values
	- **How**: Systematic hyperparameter variation while keeping specialized LLM (Llama-3.2-3B) and general LLM (Llama-3.3-70B) fixed, measuring accuracy and unique triplets generated
	- **Who**: FactKG dataset evaluated with embedding-based retrieval using BGE-Large-EN-v1.5 for dot-product similarity scoring
- Entity-Trie constraint proved crucial for entity correctness in claim verification tasks #evd-candidate ^evd-006
	- **What**: Entity Correctness metric measuring whether extracted entities from claims are valid in the knowledge graph
	- **How**: Ablation study removing Entity-Trie constraint to assess performance impact on entity validation
	- **Who**: FactKG development set evaluated with ClaimPKG framework using Llama-3B* as Specialized LLM and Llama-70B as General LLM
- Performance peaked with 3B parameter specialized models, showing optimal balance between capability and computational requirements #evd-candidate ^evd-007
	- **What**: Accuracy scores and computational requirement measurements across model sizes from 1B to 8B parameters
	- **How**: Systematic comparison of different Llama and Qwen model variants with increasing parameter counts, measuring performance improvements versus computational costs
	- **Who**: FactKG dataset evaluated using fine-tuned specialized LLMs of various sizes (1B-8B parameters) from Llama and Qwen model families
- Increasing beam size for pseudo-subgraph generation improved performance but at increased computational cost #evd-candidate ^evd-008
	- **What**: Performance accuracy measurements and computational time/cost metrics across different beam sizes (default beam size of 5 mentioned)
	- **How**: Systematic variation of beam size parameter in specialized LLM pseudo-subgraph generation while measuring both accuracy improvements and computational overhead
	- **Who**: FactKG dataset evaluated using ClaimPKG framework with specialized LLM configured for multiple pseudo-subgraph generation
- Performance declined when specialized LLMs were trained on excessive training examples, indicating need for regularization #evd-candidate ^evd-009
	- **What**: Accuracy degradation measurements when training data volume exceeded optimal amounts
	- **How**: Training experiments with varying amounts of training data to identify performance curves and overfitting patterns
	- **Who**: Specialized LLMs (fine-tuned models) trained on annotated FactKG dataset with systematic variation in training set size
