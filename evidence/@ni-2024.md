# Scientific evidence and specific context: leveraging large language models for health fact-checking

**Authors:** Zhenni Ni, Yuxing Qian, Shuaipu Chen, Marie-Christine Jaulent, CÃ©dric Bousquet
**Year:** 2024
**DOI:** 10.1108/oir-02-2024-0111
**Citation Count:** 2

## Evidence Items

- Conclusion-first prompts achieved accuracy rates of 89.70% and 66.09% for knowledge claims without retrieval augmentation, demonstrating strong baseline performance for fact-checking in the health domain. #evd-candidate ^evd-001
	- **What**: Accuracy percentages (89.70%, 66.09%) for knowledge-based health claim verification
	- **How**: Conclusion-first prompt engineering strategy evaluated without external retrieval augmentation
	- **Who**: Large Language Models tested on health fact-checking datasets containing knowledge claims
- Explanation-first prompts with retrieval augmentation significantly improved performance, achieving 87.53% and 88.60% accuracy for news claims and 87.28% and 90.62% accuracy for anecdote claims. #evd-candidate ^evd-002
	- **What**: Accuracy improvements with retrieval augmentation: news claims (87.53%, 88.60%) and anecdote claims (87.28%, 90.62%)
	- **How**: Explanation-first prompting combined with retrieval-augmented generation compared to baseline approaches
	- **Who**: LLMs evaluated on 10,212 categorized health claims from two public health fact-checking datasets
- Explanation-first prompts without retrieval augmentation frequently classified claims as 'unknown', indicating poor performance in standalone entailment tasks but revealing dependency on external evidence. #evd-candidate ^evd-003
	- **What**: High frequency of 'unknown' classifications as a performance limitation measure
	- **How**: Content analysis of LLM outputs using explanation-first prompting without external knowledge retrieval
	- **Who**: LLMs tested across knowledge, anecdote, and news claim categories in health fact-checking datasets
- Retrieval-augmented LLMs demonstrated enhanced focus on evidence and context compared to non-augmented versions, showing qualitative improvements in reasoning processes for entailment tasks. #evd-candidate ^evd-004
	- **What**: Qualitative analysis of argument elements and evidence usage patterns in LLM outputs
	- **How**: Content analysis examining reasoning processes and contextual qualifiers in generated responses
	- **Who**: LLMs with and without retrieval augmentation tested on health fact-checking claims
- Performance varied significantly across claim types, with knowledge claims showing accuracy ranges from 66.09% to 89.70%, anecdote claims from 79.49% to 90.62%, and news claims from 85.61% to 88.60%. #evd-candidate ^evd-005
	- **What**: Accuracy ranges across three claim categories: knowledge (66.09%-89.70%), anecdotes (79.49%-90.62%), news (85.61%-88.60%)
	- **How**: Comparative evaluation of different prompt strategies across categorized claim types
	- **Who**: 10,212 health claims categorized as knowledge, anecdotes, and news from two public datasets
- Cost-effectiveness analysis revealed that conclusion-first prompts provide efficient performance without requiring additional retrieval infrastructure, making them practical for resource-constrained applications. #evd-candidate ^evd-006
	- **What**: Cost-effectiveness metrics comparing computational requirements and performance trade-offs
	- **How**: Economic analysis of prompt strategies considering accuracy versus computational resource requirements
	- **Who**: LLM implementations evaluated across different prompting approaches for health fact-checking
- The study identified systematic patterns in common errors across different prompting strategies, revealing specific failure modes in AI-based entailment systems for health claims. #evd-candidate ^evd-007
	- **What**: Categorized error patterns and failure mode classifications across prompting strategies
	- **How**: Content analysis to identify and compare common errors generated by different prompt approaches
	- **Who**: LLM outputs from 10,212 health claims analyzed for systematic error patterns
