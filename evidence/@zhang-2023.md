# Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method

**Authors:** Xuan Zhang, Wei Gao
**Year:** 2023
**DOI:** 10.48550/arXiv.2310.00305
**Citation Count:** 112

## Evidence Items

- The Hierarchical Step-by-Step (HiSS) prompting method with GPT-3.5 outperformed state-of-the-art fully-supervised models by an average of 4.95% in macro-average F1 score for news claim verification. #evd-candidate ^evd-001
	- **What**: Macro-average F1 scores across three-class and six-class classification tasks, with 4.95% improvement over supervised baselines
	- **How**: Comparative evaluation using HiSS prompting method with claim decomposition and step-by-step verification against seven supervised models (CNN, RNN, etc.)
	- **Who**: GPT-3.5 (text-davinci-003) tested on RAWFC (Snopes articles) and LIAR (PolitiFact articles) datasets
- HiSS prompting substantially outperformed other few-shot prompting methods, surpassing standard prompting by 7.95%, vanilla Chain-of-Thought by 11.65%, and ReAct by 5.3% in F1 score on average. #evd-candidate ^evd-002
	- **What**: F1 score comparisons showing 7.95%, 11.65%, and 5.3% improvements over standard prompting, vanilla CoT, and ReAct respectively
	- **How**: Direct comparison of different prompting strategies using the same GPT-3.5 backbone with 4-shot in-context learning
	- **Who**: Same GPT-3.5 model tested across multiple prompting approaches on RAWFC and LIAR datasets
- LLMs relying solely on internal knowledge without external search performed poorly, achieving only 49.8% F1 score, while incorporating search improved performance to 54.4-55.4% F1. #evd-candidate ^evd-003
	- **What**: F1 scores of 49.8% (no search), 54.4% (self-decided search), and 55.4% (always search)
	- **How**: Ablation study comparing three search strategies: no search, always search, and LLM self-deciding when to search
	- **Who**: GPT-3.5 with HiSS prompting evaluated on RAWFC dataset with Google Search integration
- Claim decomposition contributed 1.5% improvement in F1 score, while step-by-step subclaim verification contributed 2.9% improvement, demonstrating the importance of both components. #evd-candidate ^evd-004
	- **What**: Performance drops of 1.5% without claim decomposition and 2.9% without step-by-step verification
	- **How**: Ablation analysis systematically removing each component of the HiSS method and measuring performance impact
	- **Who**: GPT-3.5 with modified HiSS configurations tested on RAWFC dataset
- Standard prompting with GPT-3.5 performed comparably well to strong supervised baselines, while vanilla Chain-of-Thought performed worse than standard prompting, contrary to expectations. #evd-candidate ^evd-005
	- **What**: Comparative F1 scores showing standard prompting matching supervised models while vanilla CoT underperformed
	- **How**: Direct comparison of prompting methods against established supervised baselines using macro-average precision, recall, and F1 metrics
	- **Who**: GPT-3.5 with different prompting strategies compared against seven supervised models on RAWFC and LIAR datasets
- Human evaluation revealed that HiSS-generated explanations were superior to the best supervised explainable model (CofCED) but slightly inferior to gold standard human explanations. #evd-candidate ^evd-006
	- **What**: Human ratings comparing explanation quality across three types: gold human explanations, CofCED model explanations, and HiSS explanations
	- **How**: Three human annotators independently rated 34 randomly sampled claims from LIAR test set for explanation quality
	- **Who**: Human evaluators assessing explanations generated by HiSS method, CofCED supervised model, and human journalists
- Vanilla Chain-of-Thought prompting exhibited substantial issues with fact hallucination and omission of necessary reasoning steps, which HiSS effectively addressed. #evd-candidate ^evd-007
	- **What**: Qualitative analysis identifying specific failure modes: fact hallucination and thought omission in vanilla CoT vs. successful coverage in HiSS
	- **How**: Error analysis examining reasoning trajectories and comparing coverage of explicit and implicit claim aspects across methods
	- **Who**: GPT-3.5 outputs analyzed across vanilla CoT and HiSS prompting methods on news claim verification tasks
- Few-shot in-context learning with just 4 demonstration examples enabled LLM performance comparable to fully supervised models that require extensive training data. #evd-candidate ^evd-008
	- **What**: Performance parity achieved using only 4-shot demonstrations compared to models trained on full supervised datasets
	- **How**: Hyperparameter tuning of shot numbers {1, 2, 4, 6, 8} on validation set, with optimal performance at 4 shots
	- **Who**: GPT-3.5 with optimized few-shot learning compared against CNN, RNN and other supervised models on RAWFC and LIAR
