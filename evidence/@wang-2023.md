# KnowComp at SemEval-2023 Task 7: Fine-tuning Pre-trained Language Models for Clinical Trial Entailment Identification

**Authors:** Weiqi Wang, Baixuan Xu, Tianqing Fang, Lirong Zhang, Yangqiu Song
**Year:** 2023
**DOI:** 10.18653/v1/2023.semeval-1.1
**Citation Count:** 7

## Evidence Items

- DeBERTa-v3-large achieved the highest F1 score of 0.764 on clinical trial entailment identification, significantly outperforming 7 other pre-trained language models and securing fifth rank on the official leaderboard #evd-candidate ^evd-001
	- **What**: F1 scores, precision, recall metrics, and comparative rankings across 8 different transformer models
	- **How**: Fine-tuning experiments with systematic comparison using identical training procedures, early stopping based on validation F1, and evaluation on official CodaLab platform
	- **Who**: Clinical trial dataset with 1700 training, 200 validation, and 500 test examples from real-world clinical studies database
- The best-performing model suffered from low recall (0.772, ranked 13th) despite high precision (ranked 4th), indicating systematic misclassification of entailments as contradictions #evd-candidate ^evd-002
	- **What**: Recall score of 0.772, precision ranking of 4th place, recall ranking of 13th place on leaderboard
	- **How**: Binary classification evaluation with precision/recall analysis and error pattern identification through manual inspection
	- **Who**: DeBERTa-v3-large model tested on 500-example test set from clinical trial database
- Designed prompt engineering with special token separators and evidence-after-statement ordering provided measurable performance improvements over baseline prompting approaches #evd-candidate ^evd-003
	- **What**: F1 score improvements demonstrated through ablation study comparing different prompt configurations
	- **How**: Controlled ablation experiment testing DeBERTa-v3-large with two different input prompt designs as baselines
	- **Who**: Same clinical trial entailment dataset used for primary experiments with statement-evidence pairs
- Model size and architecture significantly impacted performance, with DeBERTa-v3-large (largest parameter count) substantially outperforming smaller variants and other model families #evd-candidate ^evd-004
	- **What**: Performance metrics across models with different parameter counts, with specific comparison between DeBERTa versions
	- **How**: Systematic comparison of 8 pre-trained language models with parameter counts reported, using identical fine-tuning procedures
	- **Who**: Multiple transformer architectures (BERT, BioClinical-BERT, ALBERT, BART, RoBERTa, ELECTRA, DeBERTa, GPT2) tested on clinical dataset
- Complex mathematical reasoning and CTR premise truncation due to input length limits were identified as specific failure modes causing classification errors #evd-candidate ^evd-005
	- **What**: Error analysis revealing specific types of misclassified examples involving mathematical reasoning and truncated evidence
	- **How**: Manual inspection of system errors and analysis of cases where CTR premises exceeded 512-token input limit
	- **Who**: Error cases from the clinical trial test set containing complex reasoning or long clinical trial reports
- Masked Language Modeling (MLM) pre-training objective models generally achieved satisfactory performance on the entailment task compared to other pre-training approaches #evd-candidate ^evd-006
	- **What**: Comparative performance results showing MLM-based models outperforming generative models like GPT2
	- **How**: Cross-architectural comparison evaluating models with different pre-training objectives (MLM vs. autoregressive) under identical fine-tuning conditions
	- **Who**: 8 different pre-trained language models with varying architectures and training objectives tested on clinical entailment dataset
- Domain-specific pre-training (BioClinical-BERT) did not provide clear advantages over general-domain models for clinical trial entailment identification #evd-candidate ^evd-007
	- **What**: Performance comparison between BioClinical-BERT (medical domain pre-trained) and general BERT models
	- **How**: Direct performance comparison using identical fine-tuning procedures and evaluation metrics across domain-specific and general models
	- **Who**: Clinical trial entailment dataset comparing medical-domain BioClinical-BERT against general-purpose transformer models
- All experiments showed consistent results across three random seed repetitions, with average performance reported and no statistical significance testing conducted #evd-candidate ^evd-008
	- **What**: Averaged performance metrics across three experimental runs with different random seeds, but no confidence intervals or significance tests
	- **How**: Repeated experimentation protocol with three different random seeds and averaging of results, but lacking statistical significance analysis
	- **Who**: All 8 pre-trained language models tested with this replication protocol on the clinical trial dataset
