# Large Language Models Help Humans Verify Truthfulness – Except When They Are Convincingly Wrong

**Authors:** Chenglei Si, Navita Goyal, Sherry Tongshuang Wu, Chen Zhao, Shi Feng, Hal Daum'e, Jordan L. Boyd-Graber
**Year:** 2023
**DOI:** 10.48550/arXiv.2310.12558
**Citation Count:** 51

## Evidence Items

- ChatGPT explanations achieved 74% accuracy in helping humans verify claims, significantly better than 59% baseline but not significantly different from 73% retrieval accuracy #evd-candidate ^evd-001
	- **What**: Human verification accuracy scores: 74% ± 0.09 (explanation), 73% ± 0.12 (retrieval), 59% ± 0.12 (baseline)
	- **How**: Comparative study across conditions with statistical significance testing (z = -4.08, p = 0.00015 vs baseline; z = -0.48, p = 0.32 vs retrieval)
	- **Who**: 80 crowdworkers from Prolific verifying 200 claims from FoolMeTwice adversarial dataset
- ChatGPT explanation accuracy was 80.4% when retrieval had full recall but dropped to 67.6% when retrieval had no recall #evd-candidate ^evd-002
	- **What**: LLM explanation accuracy percentages under different retrieval conditions
	- **How**: Manual extraction and comparison of ChatGPT answers against gold labels, stratified by retrieval recall performance
	- **Who**: 200 claims from FoolMeTwice dataset processed through GTR-XXL retriever and ChatGPT (GPT-3.5-turbo-0613)
- Human accuracy was 87% when ChatGPT explanations were correct but dropped to 35% when explanations were wrong (below random chance) #evd-candidate ^evd-003
	- **What**: Human verification accuracy conditional on explanation correctness: 87% ± 0.13 (correct), 35% ± 0.22 (incorrect)
	- **How**: Post-hoc analysis comparing human performance when explanations matched vs. contradicted gold labels
	- **Who**: Subset of 80 crowdworkers' responses on claims where explanation correctness could be determined
- ChatGPT explanations enabled 2.5x faster claim verification compared to search engine results while maintaining similar accuracy #evd-candidate ^evd-004
	- **What**: Time per claim verification: 1.01 ± 0.45 minutes (explanation) vs 2.53 ± 1.07 minutes (retrieval)
	- **How**: Automated timing measurement during human verification tasks with statistical significance testing (z = -5.09, p = 9.1e-6)
	- **Who**: 80 crowdworkers from Prolific completing claim verification tasks
- Contrastive explanations improved human accuracy to 56% when non-contrastive explanations were wrong, compared to 35% with standard explanations #evd-candidate ^evd-005
	- **What**: Human accuracy on incorrectly explained claims: 56% ± 0.24 (contrastive) vs 35% ± 0.22 (non-contrastive)
	- **How**: Controlled comparison between explanation types using statistical testing (z = -2.52, p = 0.009)
	- **Who**: Subset of crowdworkers evaluating claims where non-contrastive explanations were determined to be incorrect
- Combining retrieval and explanation provided no significant accuracy improvement over either method alone #evd-candidate ^evd-006
	- **What**: Human accuracy in combined condition showed no significant difference from individual methods
	- **How**: Statistical comparison across explanation-only, retrieval-only, and combined conditions using z-tests
	- **Who**: 16 annotators per condition from the 80 total crowdworkers recruited from Prolific
- GTR-XXL retriever achieved 81.5% full recall and 93.0% partial recall on Wikipedia evidence retrieval for adversarial claims #evd-candidate ^evd-007
	- **What**: Retrieval performance metrics: 81.5% full recall (all evidence sentences), 93.0% partial recall (at least one evidence sentence)
	- **How**: Evaluation of top-10 retrieved passages against gold evidence annotations requiring multiple sentences for verification
	- **Who**: 200 claims from FoolMeTwice dataset requiring at least two Wikipedia sentences for verification
- Human verification accuracy on adversarial claims without AI assistance was only 59%, indicating inherent task difficulty #evd-candidate ^evd-008
	- **What**: Baseline human accuracy of 59% ± 0.12 on claim verification without any AI assistance
	- **How**: Control condition measurement where participants made binary true/false decisions using only the claim text
	- **Who**: 16 crowdworkers from Prolific evaluating 20 claims each from the adversarial FoolMeTwice dataset
