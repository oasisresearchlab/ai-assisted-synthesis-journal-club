# Text Alignment Is An Efficient Unified Model for Massive NLP Tasks

**Authors:** Yuheng Zha, Yichi Yang, Ruichen Li, Zhiting Hu
**Year:** 2023
**DOI:** 10.48550/arXiv.2307.02729
**Citation Count:** 13

## Evidence Items

- ALIGN (355M parameters) achieved comparable performance to FLAN-T5 models that are 8.5x larger (3B parameters) on entailment and related tasks #evd-candidate ^evd-001
	- **What**: Average accuracy scores across 20 in-domain datasets including textual entailment, fact verification, and semantic similarity tasks
	- **How**: Comparative evaluation using standardized test sets with cross-entropy loss for classification tasks and mean squared error for regression tasks
	- **Who**: ALIGN model vs FLAN-T5 variants (220M to 3B parameters) tested on 20 datasets including MultiNLI, FEVER, VitaminC, and DocNLI
- ALIGN outperformed GPT-3.5 and sometimes GPT-4 on factual consistency evaluation across 23 datasets #evd-candidate ^evd-002
	- **What**: ROC AUC scores for binary classification of factual consistency, with ALIGN beating GPT-3.5 on QAGS-XSum and QAGS-CNNDM datasets
	- **How**: Binary classification evaluation treating each model as a classifier for detecting factual consistency errors using ROC AUC metrics
	- **Who**: ALIGN (125M and 355M variants) vs GPT-3.5, GPT-4, G-EVAL, and GPTScore on TRUE (11 datasets) and SummaC (6 datasets) benchmarks
- ALIGN achieved zero-shot performance comparable to FLAN-T5 on 9 unseen entailment datasets, even on datasets present in FLAN-T5's training set #evd-candidate ^evd-003
	- **What**: Average accuracy scores on zero-shot evaluation across 9 previously unseen datasets
	- **How**: Zero-shot evaluation without task-specific fine-tuning, comparing models of similar parameter sizes using standard accuracy metrics
	- **Who**: ALIGN vs similarly-sized FLAN-T5 variants tested on 9 unseen datasets not used during ALIGN training
- When used as a verifier add-on, ALIGN improved GPT-3.5's question answering performance by 17.94 points in exact match score and 15.05 points in F1 score #evd-candidate ^evd-004
	- **What**: Exact match scores and macro-averaged F1 scores for question answering, plus ROC AUC for identifying unanswerable questions
	- **How**: Ensemble approach using ALIGN as a verifier to identify unanswerable questions, with performance measured on answerable vs unanswerable question classification
	- **Who**: GPT-3.5 and FLAN-T5 models enhanced with ALIGN verifier, tested on SQuAD v2 and ACE-whQA datasets
- ALIGN outperformed task-specific fine-tuned models and multi-task RoBERTa models on entailment tasks despite using a unified training approach #evd-candidate ^evd-005
	- **What**: Average accuracy scores comparing unified model against models specifically fine-tuned for individual tasks
	- **How**: Comparative evaluation against baseline models including multi-task RoBERTa trained on original datasets and instruction fine-tuned T5-base on alignment datasets
	- **Who**: ALIGN vs task-specific fine-tuned models and multi-task RoBERTa baseline across multiple NLI and fact verification datasets
- ALIGN maintained strong performance on three-way classification tasks (entailment, contradiction, neutral) across multiple NLI datasets with varying complexity #evd-candidate ^evd-006
	- **What**: Classification accuracy on three-way entailment tasks with labels mapped to 'entailment', 'contradiction', and 'neutral'
	- **How**: Cross-entropy loss training with three classification heads, evaluated using standard accuracy metrics on test sets
	- **Who**: MultiNLI (393k examples), Adversarial NLI (163k examples), DocNLI, and NLI-style FEVER datasets (208k examples)
- ALIGN showed effectiveness across different entailment task formats, handling both discrete classification and continuous similarity scoring #evd-candidate ^evd-007
	- **What**: Average Pearson correlation coefficients for semantic textual similarity tasks and accuracy scores for classification tasks
	- **How**: Dual training approach using both classification heads (cross-entropy loss) and regression heads (mean squared error loss) depending on task requirements
	- **Who**: Mixed evaluation on STS tasks using correlation metrics and NLI/fact verification tasks using accuracy metrics across 28 training datasets
