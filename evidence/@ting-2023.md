# Improving Chinese Fact Checking via Prompt Based Learning and Evidence Retrieval

**Authors:** Yu-Yen Ting, Chia-Hui Chang
**Year:** 2023
**DOI:** 10.1145/3625007.3629126
**Citation Count:** 0

## Evidence Items

- Prompt-based learning improved claim verification performance by 2-3 percentage points, achieving macro-F1 of 80.29% compared to 77.62% baseline when using golden evidence. #evd-candidate ^evd-001
	- **What**: Macro-F1 scores for claim verification task, showing improvement from 77.62% to 80.29%
	- **How**: Comparative evaluation using prompt-based learning with Automated Prompt Engineering (APE), prompt tuning, and LoRA strategies against baseline approach
	- **Who**: 110M BERT-based model tested on Chinese CHEF dataset with golden evidence pairs
- SentenceBERT dramatically outperformed PromptBERT for evidence retrieval, achieving 88.15% micro-F1 versus 30.61% micro-F1, both substantially better than 11.86% baseline. #evd-candidate ^evd-002
	- **What**: Micro-F1 scores for evidence retrieval: baseline 11.86%, PromptBERT 30.61%, SentenceBERT 88.15%
	- **How**: Comparative evaluation of supervised SentenceBERT versus unsupervised PromptBERT models against semantic ranking-based baseline
	- **Who**: Chinese CHEF dataset used for evidence retrieval component of fact-checking pipeline
- End-to-end fact-checking performance showed substantial improvement from 61.94% to 80.16% macro-F1 when replacing semantic ranking with SentenceBERT for evidence retrieval. #evd-candidate ^evd-003
	- **What**: Overall fact-checking macro-F1 performance metrics showing 18.22 percentage point improvement
	- **How**: System-level evaluation comparing complete fact-checking pipeline with different evidence retrieval components
	- **Who**: Chinese CHEF dataset evaluated across full fact-checking task including both evidence retrieval and claim verification
- BERT-based models achieved over 80% macro-F1 performance on Chinese claim verification when enhanced with prompt-based learning techniques. #evd-candidate ^evd-004
	- **What**: Absolute performance level of 80.29% macro-F1 for claim verification task
	- **How**: Implementation of prompt-based learning approaches including automated prompt engineering and parameter-efficient fine-tuning methods
	- **Who**: 110M parameter BERT-based model evaluated on Chinese fact-checking claims from CHEF dataset
- Evidence retrieval quality showed massive variability across different AI approaches, with performance ranging from 11.86% to 88.15% micro-F1. #evd-candidate ^evd-005
	- **What**: Wide performance range spanning 76.29 percentage points across different evidence retrieval methods
	- **How**: Direct comparison of baseline semantic ranking, unsupervised PromptBERT, and supervised SentenceBERT approaches
	- **Who**: Chinese CHEF dataset evidence retrieval subtask with multiple model architectures tested
- Prompt-based learning techniques consistently improved performance across both evidence retrieval and claim verification components of the fact-checking pipeline. #evd-candidate ^evd-006
	- **What**: Performance improvements observed in both pipeline components: 2-3% for claim verification and 158-643% relative improvement for evidence retrieval
	- **How**: Multi-component evaluation using prompt-based learning methods applied to both evidence retrieval and claim verification stages
	- **Who**: Chinese CHEF dataset with comprehensive evaluation across complete fact-checking workflow
