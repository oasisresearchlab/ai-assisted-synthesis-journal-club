# MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents

**Authors:** Liyan Tang, Philippe Laban, Greg Durrett
**Year:** 2024
**DOI:** 10.48550/arXiv.2404.10774
**Citation Count:** 138

## Evidence Items

- MiniCheck-FT5 (770M parameters) achieved performance equivalent to GPT-4 while being 400 times cheaper in computational cost #evd-candidate ^evd-001
	- **What**: Performance parity with GPT-4 and 400x cost reduction measured through inference cost comparison
	- **How**: Comparative evaluation on LLM-AggreFact benchmark with computational cost analysis using GPU time converted to cloud computing equivalent costs vs API call costs
	- **Who**: MiniCheck-FT5 model vs GPT-4 tested on unified benchmark of 10 fact-checking datasets
- MiniCheck-FT5 achieved a 4.3% overall improvement over AlignScore, outperforming it on 6 out of 10 datasets #evd-candidate ^evd-002
	- **What**: 4.3% performance improvement and superior performance on 60% of test datasets
	- **How**: Head-to-head comparison using binary classification accuracy on claim-evidence pairs across multiple datasets
	- **Who**: MiniCheck-FT5 vs AlignScore (previous SOTA specialized fact-checker) on LLM-AggreFact benchmark datasets
- Synthetic data training improved performance across three different model architectures (RoBERTa, DeBERTa, and Flan-T5) #evd-candidate ^evd-003
	- **What**: Performance gains demonstrated across diverse transformer architectures when trained on synthetic data
	- **How**: Ablation study comparing models trained with and without synthetic data using same evaluation protocol
	- **Who**: MiniCheck-RBTA (RoBERTa), MiniCheck-DBTA (DeBERTa), and MiniCheck-FT5 (Flan-T5) models
- Claim decomposition into atomic facts showed near-zero performance change for GPT-4 and mixed results for specialized fact-checkers #evd-candidate ^evd-004
	- **What**: Minimal to no performance improvement when breaking claims into smaller atomic units
	- **How**: Comparative analysis using claim decomposition prompts vs direct claim verification, measuring factuality prediction accuracy
	- **Who**: GPT-4 and subset of specialized fact-checkers tested on decomposed vs non-decomposed claims
- MiniCheck models (RBTA and DBTA) surpassed non-frontier LLM-based fact-checkers by large margins #evd-candidate ^evd-005
	- **What**: Substantial performance gaps favoring smaller specialized models over larger general-purpose LLMs
	- **How**: Comparative evaluation using standardized fact-checking accuracy metrics across model types
	- **Who**: MiniCheck-RBTA and MiniCheck-DBTA vs non-frontier LLMs (Gemini-Pro, PaLM2-Bison, Mistral variants, Claude 2.1, GPT-3.5)
- Specialized fact-checking models demonstrated much lower inference costs compared to LLM-based approaches #evd-candidate ^evd-006
	- **What**: Computational cost measurements showing specialized models require significantly fewer resources
	- **How**: Cost analysis converting GPU prediction time to cloud computing equivalents for specialized models vs API call costs for LLMs
	- **Who**: All specialized fact-checkers (T5-NLI-Mixed, DAE, QAFactEval, SummaC variants, AlignScore, MiniCheck models) vs LLM-based fact-checkers
- Decontextualization (removing coreference and ellipsis) did not improve fact-checking performance on the benchmark #evd-candidate ^evd-007
	- **What**: No performance gains from preprocessing claims to remove contextual dependencies
	- **How**: Comparison of fact-checking accuracy with and without decontextualization preprocessing step using specialized prompts
	- **Who**: Models tested on LLM-AggreFact benchmark with claims processed through decontextualization vs original claims
- MiniCheck-FT5 achieved performance equivalent to Claude-3 Opus despite having much smaller model size #evd-candidate ^evd-008
	- **What**: Performance parity between 770M parameter model and much larger frontier LLM
	- **How**: Direct performance comparison using same evaluation metrics and datasets
	- **Who**: MiniCheck-FT5 (770M parameters) vs Claude-3 Opus on LLM-AggreFact benchmark
- Models trained on simplified synthetic data construction failed to develop desired reasoning properties #evd-candidate ^evd-009
	- **What**: Performance degradation when using simplified synthetic data generation methods (C2D-SIMP and D2C-SIMP)
	- **How**: Ablation study comparing models trained on full synthetic data pipeline vs simplified versions
	- **Who**: MiniCheck model variants trained on different synthetic data construction approaches
