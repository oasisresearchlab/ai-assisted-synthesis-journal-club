# FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation

**Authors:** Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, C. Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, Thang Luong
**Year:** 2023
**DOI:** 10.48550/arXiv.2310.03214
**Citation Count:** 255

## Evidence Items

- FRESHPROMPT method significantly improved AI system accuracy on claim verification tasks, with GPT-4 achieving 32.6% improvement under RELAXED evaluation and 49.0% improvement under STRICT evaluation compared to vanilla models. #evd-candidate ^evd-001
	- **What**: Accuracy improvements measured under two evaluation modes - RELAXED (main answer correctness) and STRICT (all claims factual and up-to-date)
	- **How**: Comparative evaluation using search engine-augmented prompting method vs. vanilla prompting on the same models
	- **Who**: GPT-4 and GPT-3.5 models tested on FreshQA dataset of 600 questions
- All tested AI models, regardless of size (770M to 540B parameters), struggled significantly with false-premise questions, achieving only 0.0% to 1.6% accuracy for T5 and PALM models on questions requiring premise debunking. #evd-candidate ^evd-002
	- **What**: Accuracy scores on false-premise questions that require identifying invalid assumptions before answering
	- **How**: Binary classification evaluation where models must correctly identify and debunk false premises to receive credit
	- **Who**: Nine different models (T5, PALM, PALMCHILLA, FLAN-T5, FLAN-PALM, GPT-3.5, CODEX, CHATGPT, GPT-4) on FreshQA false-premise question subset
- Modern LLMs (GPT-3.5, ChatGPT, GPT-4) demonstrated superior performance on entailment tasks compared to other models, achieving 25.8% to 42.7% accuracy under STRICT evaluation and 32.3% to 66.9% under RELAXED evaluation. #evd-candidate ^evd-003
	- **What**: Accuracy percentages under both STRICT (no hallucination) and RELAXED (main answer correct) evaluation modes
	- **How**: Human evaluation involving over 50K judgments using two-mode evaluation procedure with 99% inter-annotator agreement
	- **Who**: OpenAI models (GPT-3.5, ChatGPT, GPT-4) compared against T5, PALM, and FLAN variants on 600 FreshQA questions
- Chain-of-Thought (COT) prompting increased hallucination in AI systems performing claim verification, despite providing reasoning steps for complex questions. #evd-candidate ^evd-004
	- **What**: Hallucination rates measured as the gap between RELAXED and STRICT evaluation scores
	- **How**: Comparative analysis of few-shot prompting vs. Chain-of-Thought prompting approaches on the same models
	- **Who**: Large language models tested on FreshQA questions with valid premises, particularly those requiring multi-hop reasoning
- Adding premise verification checks improved AI performance on false-premise questions by +23.4% for GPT-3.5 and +6.4% for GPT-4 under STRICT evaluation, but hurt performance on questions with valid premises. #evd-candidate ^evd-005
	- **What**: Accuracy improvements and decrements measured separately for false-premise vs. valid-premise question subsets
	- **How**: A/B testing with and without explicit premise checking instructions ('Please check if the question contains a valid premise before answering')
	- **Who**: GPT-3.5 and GPT-4 models evaluated on FreshQA dataset partitioned by premise validity
- The number of retrieved evidence pieces significantly impacted entailment task performance, with accuracy improving by +9.2%, +14.2%, and +16.2% when increasing evidence count from 1 to 5, 10, and 15 pieces respectively. #evd-candidate ^evd-006
	- **What**: Absolute accuracy improvements under STRICT evaluation mode as evidence quantity increased
	- **How**: Controlled experiments varying the number of search engine results incorporated into prompts while keeping other factors constant
	- **Who**: GPT-4 with FRESHPROMPT method tested on FreshQA dataset with different evidence quantities
- Search engine-augmented AI systems outperformed competing methods by substantial margins, with GPT-4 + FRESHPROMPT surpassing Self-Ask prompting and commercial Perplexity.AI system across all question types. #evd-candidate ^evd-007
	- **What**: Comparative accuracy scores across different search-augmented approaches and commercial systems
	- **How**: Head-to-head comparison using identical evaluation protocol and dataset across multiple search-augmented methods
	- **Who**: GPT-4 + FRESHPROMPT vs. Self-Ask method vs. Perplexity.AI commercial system, all evaluated on FreshQA benchmark
- Model size scaling showed flat performance curves on fast-changing knowledge questions, indicating that simply increasing parameters (770M to 540B) did not reliably improve entailment task effectiveness for time-sensitive claims. #evd-candidate ^evd-008
	- **What**: Accuracy scores on questions requiring current/recent world knowledge across different model parameter sizes
	- **How**: Cross-model comparison analyzing performance vs. parameter count relationship on temporally-sensitive question subset
	- **Who**: T5 and PALM model families ranging from 770M to 540B parameters evaluated on FreshQA fast-changing knowledge questions
